{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "384217ad-9664-47e1-9d7e-7ce13b9b3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a5176c-bb0d-40ad-b5da-e7f6b51cded6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Path to the image directory and annotation file\n",
    "dataset_path = '../data'\n",
    "anns_file_path = dataset_path + '/' + 'annotations.json'\n",
    "\n",
    "# Load the annotations\n",
    "coco = COCO(anns_file_path)\n",
    "\n",
    "# Create a dictionary of the label names and their corresponding IDs\n",
    "labels = {}\n",
    "for i, cat in enumerate(coco.loadCats(coco.getCatIds())):\n",
    "    labels[cat['name']] = i\n",
    "    \n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496d753b-1c32-4ff8-ac14-e9800c270949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# specify the directory path\n",
    "dir_path = '/path/to/directory'\n",
    "\n",
    "# list all the files in the directory\n",
    "file_list = os.listdir(dir_path)\n",
    "\n",
    "# select the second file name\n",
    "second_file = file_list[1]\n",
    "\n",
    "print(second_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "eb977db9-e050-4375-a867-cc50f5f1b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = '/Users/mjs/Desktop/Dev/TACO/data/all_images'\n",
    "\n",
    "# Set the batch size and image size\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "# Load the image file names and labels from a CSV file\n",
    "df = pd.read_csv('../data/filtered_labels.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "32a59cf2-f95a-4b61-8bab-d3a087310b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 833 validated image filenames.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DataFrameIterator at 0x7fea36ec1e70>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen_train=tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                      rescale=1./255,\n",
    "                      rotation_range=15,\n",
    "                      width_shift_range=0.2,\n",
    "                      height_shift_range=0.2,\n",
    "                      shear_range=0.005,\n",
    "                      zoom_range=[0.9, 1.4],\n",
    "                      horizontal_flip=True,\n",
    "                      vertical_flip=False,\n",
    "                      brightness_range=(.8,1.2),\n",
    "                      fill_mode='nearest',\n",
    "                      )\n",
    "\n",
    "train_generator=datagen_train.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=IMAGE_DIR,\n",
    "    shuffle=True,\n",
    "    x_col='file_name',\n",
    "    y_col=['Aluminium_blister_pack', 'Carded_blister_pack', 'Other_plastic_bottle', 'Clear_plastic_bottle'],\n",
    "  # save_to_dir=savepath + '/aug_images', \n",
    "    #classes=train_classes,\n",
    "    class_mode='multi_output',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE), \n",
    "    batch_size=batch_size)\n",
    "\n",
    "train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8ffdb1c6-fe68-4d01-8140-d2b56e7683f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/filtered_labels.csv')\n",
    "df.head()\n",
    "type(False)\n",
    "type(df['Aluminium_foil'][0])\n",
    "df = df.astype(str)\n",
    "type(df['Aluminium_foil'][0])\n",
    "\n",
    "labels = list(df.columns)[1:]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "15bd8833-0bd2-4903-8b2f-39523d52f622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 666 validated image filenames.\n",
      "Found 167 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mjs/Conda/anaconda3/envs/DLG/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "111/111 [==============================] - 138s 1s/step - loss: 0.2231 - accuracy: 0.0796 - val_loss: 0.1236 - val_accuracy: 0.0060\n",
      "Epoch 2/10\n",
      "111/111 [==============================] - 139s 1s/step - loss: 0.1225 - accuracy: 0.1036 - val_loss: 0.1216 - val_accuracy: 0.1377\n",
      "Epoch 3/10\n",
      "111/111 [==============================] - 137s 1s/step - loss: 0.1216 - accuracy: 0.0811 - val_loss: 0.1214 - val_accuracy: 0.0060\n",
      "Epoch 4/10\n",
      "111/111 [==============================] - 137s 1s/step - loss: 0.1214 - accuracy: 0.0961 - val_loss: 0.1215 - val_accuracy: 0.1377\n",
      "Epoch 5/10\n",
      "111/111 [==============================] - 141s 1s/step - loss: 0.1212 - accuracy: 0.0871 - val_loss: 0.1219 - val_accuracy: 0.1377\n",
      "Epoch 6/10\n",
      "111/111 [==============================] - 138s 1s/step - loss: 0.1216 - accuracy: 0.1021 - val_loss: 0.1222 - val_accuracy: 0.1377\n",
      "Epoch 7/10\n",
      "111/111 [==============================] - 143s 1s/step - loss: 0.1212 - accuracy: 0.1036 - val_loss: 0.1229 - val_accuracy: 0.0060\n",
      "Epoch 8/10\n",
      "111/111 [==============================] - 142s 1s/step - loss: 0.1215 - accuracy: 0.0931 - val_loss: 0.1220 - val_accuracy: 0.0060\n",
      "Epoch 9/10\n",
      "111/111 [==============================] - 137s 1s/step - loss: 0.1213 - accuracy: 0.0976 - val_loss: 0.1215 - val_accuracy: 0.1377\n",
      "Epoch 10/10\n",
      "111/111 [==============================] - 139s 1s/step - loss: 0.1215 - accuracy: 0.0916 - val_loss: 0.1221 - val_accuracy: 0.0060\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define hyperparameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 6\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# Load data from CSV file\n",
    "df = pd.read_csv('../data/filtered_labels.csv')\n",
    "\n",
    "labels = list(df.columns)[1:]\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Define data generators for train and validation sets\n",
    "datagen_train = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.005,\n",
    "    zoom_range=[0.9, 1.4],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    brightness_range=(.8,1.2),\n",
    "    fill_mode='nearest',\n",
    ")\n",
    "\n",
    "datagen_val = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    ")\n",
    "\n",
    "train_generator = datagen_train.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory='/Users/mjs/Desktop/Dev/TACO/data/all_images',\n",
    "    x_col='file_name',\n",
    "    y_col=labels,\n",
    "    class_mode='raw',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_generator = datagen_val.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory='/Users/mjs/Desktop/Dev/TACO/data/all_images',\n",
    "    x_col='file_name',\n",
    "    y_col=labels,\n",
    "    class_mode='raw',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Load the pre-trained EfficientNetB0 model and its weights\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# Freeze all the layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a custom classification head\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(len(labels), activation='sigmoid')(x)\n",
    "\n",
    "# Create the final model by combining the base model with the custom head\n",
    "model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(lr=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "# Train the model\n",
    "history = model.fit(train_generator,\n",
    "                    validation_data=val_generator,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    #callbacks=[checkpoint]\n",
    "                   )\n",
    "\n",
    "# Convert the history object to a dictionary\n",
    "history_dict = history.history\n",
    "# Convert any numpy arrays to lists so they can be serialized by JSON\n",
    "for key, val in history_dict.items():\n",
    "    if isinstance(val, np.ndarray):\n",
    "        history_dict[key] = val.tolist()\n",
    "# Save the history dictionary to a JSON file\n",
    "with open('history.json', 'w') as outfile:\n",
    "    json.dump(history_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "824d0d91-6402-4b53-bd6e-6e472fe61941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 25s 894ms/step - loss: 0.1221 - accuracy: 0.0060\n",
      "Validation loss: 0.122\n",
      "Validation accuracy: 0.006\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "loss, accuracy = model.evaluate(val_generator)\n",
    "print(f'Validation loss: {loss:.3f}')\n",
    "print(f'Validation accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0d67f-349c-4c62-97e0-93bdde945117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf86a5-5967-45db-a0ab-b6d47f226e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1266c-7bb8-408a-a15b-570d8ee6e249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLG",
   "language": "python",
   "name": "dlg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

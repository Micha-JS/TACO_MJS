{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3eb102-a815-4342-b24f-221517a66073",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "This Notebook helps to transform the data to my desired format. Apply this Notebook here (https://github.com/pedropro/TACO) to create a .csv with all the labels and the corresponding image names.\n",
    "\n",
    "The images are divided in 15 folders with ambiguous names, this problem is resolved by adding the folder name (batch_*) as a prefix. The images are also saved in this format in the annotations.json\n",
    "\n",
    "\n",
    "#### Section 1\n",
    "Create a csv file with the labels and the images\n",
    "\n",
    "#### Section 2\n",
    "Rename the images (unique names) and create one folder with all the images\n",
    "\n",
    "#### Section 3\n",
    "Feature engineering\n",
    "\n",
    "#### Section 4\n",
    "Split and balance the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33ce029-37a2-4a77-8973-5a76563c9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5227f9b-128d-4cfe-91af-695cc68e9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the image directory and annotation file\n",
    "dataset_path = './data'\n",
    "anns_file_path = dataset_path + '/' + 'splits/annotations.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fddf246a-d9fa-45a4-9539-3b993aa623d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 60\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "with open(anns_file_path, 'r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "    \n",
    "anns = dataset['annotations']\n",
    "imgs = dataset['images']\n",
    "categories = dataset['categories']\n",
    "scenes = dataset['scene_annotations']\n",
    "scene_cat = dataset['scene_categories']\n",
    "\n",
    "df_anns_raw = pd.DataFrame(anns)\n",
    "df_imgs = pd.DataFrame(imgs)\n",
    "scenes = pd.DataFrame(scenes)\n",
    "\n",
    "print(len(scene_cat), len(categories))\n",
    "\n",
    "categories[1]\n",
    "\n",
    "unique_supercategories = set()\n",
    "\n",
    "for my_dict in categories:\n",
    "    unique_supercategories.add(my_dict['supercategory'])\n",
    "\n",
    "print(len(unique_supercategories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c724ed38-1849-4169-b8a2-15f4762460b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1496, 2)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenes = dataset['scene_annotations']\n",
    "scene_cat = dataset['scene_categories']\n",
    "\n",
    "df_scenes = pd.DataFrame(scenes)\n",
    "\n",
    "df_labels.columns\n",
    "df_scenes['image_id'].value_counts()\n",
    "df_scenes[df_scenes['image_id']==603]\n",
    "df_scenes = df_scenes.drop_duplicates(subset='image_id')\n",
    "df_scenes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73e1c4-14ca-452e-93de-578e9bb16a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cc4afe1-a823-4de9-8b88-ce532b6e9601",
   "metadata": {},
   "source": [
    "#### Section 1: Create the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d3901baa-9a06-4f24-bca6-cad3d43b0cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 2) (1500, 9)\n"
     ]
    }
   ],
   "source": [
    "# Creating a csv File with the image names and the labels\n",
    "# Merge two df, create an array with 0/1 depending on the labels\n",
    "# categories_level = 'name' for all 60 labels and categories_level = 'supercategory' for the reduced 28 labels\n",
    "# add_scene = 'scene' if you want to add the scene annotations\n",
    "def create_image_label_df(anns_file_path, categories_level = 'name', add_scene = 'scene'):\n",
    "    with open(anns_file_path, 'r') as f:\n",
    "        dataset = json.loads(f.read())\n",
    "\n",
    "    anns = dataset['annotations']\n",
    "    imgs = dataset['images']\n",
    "    categories = dataset['categories']\n",
    "    scenes = dataset['scene_annotations']\n",
    "    scene_cat = dataset['scene_categories']\n",
    "\n",
    "\n",
    "    df_anns_raw = pd.DataFrame(anns)\n",
    "    df_imgs = pd.DataFrame(imgs)\n",
    "    df_scenes = pd.DataFrame(scenes)\n",
    "    df_scenes = df_scenes.drop_duplicates(subset='image_id')\n",
    "    \n",
    "\n",
    "    df_anns = df_anns_raw.groupby('image_id').agg({'category_id':list}).reset_index()\n",
    "\n",
    "    df_anns['category_id'] = df_anns['category_id'].apply(set)\n",
    "    df_scenes['background_ids'] = df_scenes['background_ids'].apply(set)\n",
    "    \n",
    "    \n",
    "    # Add the file names to the category IDs - you can add more columns like the boxes by adding the column names\n",
    "    img_label_df = pd.merge(df_anns[['image_id', 'category_id']], \n",
    "                            df_imgs[['id', 'file_name']], \n",
    "                            left_on='image_id',\n",
    "                            right_on = 'id')\n",
    "    \n",
    "   \n",
    "    # One hot encode all labels with 0 or 1 for all the categories\n",
    "    array_labels = lambda x: np.array([1 if i in x else 0 for i in range(len(categories))])\n",
    "    img_label_df['labels'] = img_label_df['category_id'].apply(array_labels)\n",
    "\n",
    "\n",
    "    #Change the array to columns\n",
    "    to_dict = lambda x: dict(zip([f'{i}' for i in range(len(x))], x))\n",
    "    new_df = pd.DataFrame(img_label_df['labels'].apply(to_dict).tolist())\n",
    "\n",
    "    img_label_df = pd.concat([img_label_df.drop('labels', axis=1), new_df], axis=1)\n",
    "\n",
    "    # Rename the columns according to the category\n",
    "    if categories_level == 'supercategory':\n",
    "        cat = {str(d['id']): d['supercategory'].replace(' ', '_') for d in categories}\n",
    "        img_label_df = img_label_df.rename(columns = cat)\n",
    "    else:\n",
    "        cat = {str(d['id']): d['name'].replace(' ', '_') for d in categories}\n",
    "        img_label_df = img_label_df.rename(columns = cat)\n",
    "    \n",
    "    \n",
    "    img_label_df = img_label_df.drop(['category_id','id'], axis = 1)\n",
    "    img_label_df['file_name'] = img_label_df['file_name'].str.replace('/', '_')\n",
    "    \n",
    "    img_label_df['file_name'] = img_label_df['file_name'].str.lower()\n",
    "\n",
    "\n",
    " \n",
    "    # Do the same for scenes\n",
    "    if add_scene == 'scene':\n",
    "        col_temp = img_label_df.columns\n",
    "        \n",
    "        img_labels_scene_df = pd.merge(img_label_df[col_temp], \n",
    "                                df_scenes[['image_id', 'background_ids']], \n",
    "                                left_on='image_id',\n",
    "                                right_on = 'image_id')\n",
    "        \n",
    "        # One hot encode all labels with 0 or 1 for all the categories\n",
    "        array_scenes = lambda x: np.array([1 if i in x else 0 for i in range(len(scene_cat))])\n",
    "        img_labels_scene_df['scenes'] = img_labels_scene_df['background_ids'].apply(array_scenes)\n",
    "        \n",
    "        #Change the array to columns\n",
    "        to_dict = lambda x: dict(zip([f'{i}' for i in range(len(x))], x))\n",
    "        new_df = pd.DataFrame(img_labels_scene_df['scenes'].apply(to_dict).tolist())\n",
    "        img_labels_scene_df = pd.concat([img_labels_scene_df.drop('scenes', axis=1), new_df], axis=1)\n",
    "        \n",
    "        # Rename the columns according to the scenes\n",
    "        cat = {str(d['id']): d['name'].replace(' ', '_') for d in scene_cat}\n",
    "        img_labels_scene_df = img_labels_scene_df.rename(columns = cat)\n",
    "        \n",
    "        img_labels_scene_df = img_labels_scene_df.drop(['image_id','background_ids'], axis = 1)\n",
    "        img_label_df = img_labels_scene_df\n",
    "    else:\n",
    "        add_scene = '_'\n",
    "        print('No scene annotations added')\n",
    "\n",
    "        \n",
    "        \n",
    "    # Drop columns that are double, and create a binary df again\n",
    "    if categories_level == 'supercategory':\n",
    "        img_label_df = img_label_df.groupby(img_label_df.columns, axis=1).sum()\n",
    "        \n",
    "        def binarize(x):\n",
    "            if x == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        img_label_df.iloc[:,:-1] = img_label_df.iloc[:,:-1].applymap(binarize)\n",
    "        \n",
    "        #If you use this, personalize your directories\n",
    "        dir_path = 'data/splits/'\n",
    "        df_name = f'sup_cat_{add_scene}_labels.csv'\n",
    "        dir_df = dir_path+df_name\n",
    "        img_label_df.to_csv(dir_df,index=False)\n",
    "    else:\n",
    "        dir_path = 'data/splits/'\n",
    "        df_name = f'{add_scene}_labels.csv'\n",
    "        dir_df = dir_path+df_name\n",
    "        img_label_df.to_csv(dir_df,index=False)\n",
    "        \n",
    "\n",
    "    \n",
    "    return img_label_df\n",
    "\n",
    "df = create_image_label_df(anns_file_path, categories_level = 'supercategory',add_scene = 'scene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f550d-08d7-4c05-8c2d-053820f42c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a87c913-d3a6-44f7-a305-f519dea7a544",
   "metadata": {},
   "source": [
    "#### Section 2: Changes on the folder structure of the images & image name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd400e34-e550-4c93-aaef-c26b5d636338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the image names that they match the csv file. Use this function in the GitHub TACO data folder\n",
    "\n",
    "directory_path = '../data'\n",
    "folder_names = ['batch_' + str(i) for i in range(1, 16)]\n",
    "\n",
    "def add_folder_prefix_to_image_names(directory_path, folder_names):\n",
    "    \"\"\"\n",
    "    Adds the folder name as a prefix to the image names in each folder in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): The path to the directory containing the folders.\n",
    "        folder_names (list of str): A list of folder names in the directory.\n",
    "    \"\"\"\n",
    "    for folder_name in folder_names:\n",
    "        folder_path = os.path.join(directory_path, folder_name)\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            new_file_name = folder_name + '_' + file_name\n",
    "            os.rename(file_path, os.path.join(folder_path, new_file_name))\n",
    "\n",
    "\n",
    "#add_folder_prefix_to_image_names(directory_path, folder_names)\n",
    "\n",
    "# Unify all the image names (lower-case)\n",
    "\n",
    "directory_path = '/Users/mjs/Desktop/Dev/TACO/data/all_images'\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".JPG\"):\n",
    "        new_filename = filename.lower()\n",
    "        os.rename(os.path.join(directory_path, filename), os.path.join(directory_path, new_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf59d81b-8db1-480a-a369-959b6e6bd7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the new folder that contains all the images\n",
    "def collect_images(directory_path, folder_names):\n",
    "    new_folder_path = os.path.join(directory_path, 'all_images')\n",
    "    if not os.path.exists(new_folder_path):\n",
    "        os.mkdir(new_folder_path)\n",
    "\n",
    "    for folder_name in folder_names:\n",
    "        folder_path = os.path.join(directory_path, folder_name)\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            shutil.copy(file_path, os.path.join(new_folder_path, file_name))\n",
    "            \n",
    "            \n",
    "#collect_images(directory_path, folder_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eacd7d-c91d-4d34-a1ea-8d26a88314f9",
   "metadata": {},
   "source": [
    "#### Section 3: Feature engineering\n",
    "There are 60 categories and 28 super categories given. \n",
    "\n",
    "For easier use I choose new labels for the dataset, namely packaging materials (packaging_recycle), paper, glass, others and toxic materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0573176d-2223-4a84-a4aa-9442ca2ede2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1496, 36)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('./data/splits/sup_cat_scene_labels.csv')\n",
    "labels.columns\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7e703611-9a60-4d2c-8459-aa958ba9365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g0/54cppbx96dj9w8tzkq98nqsw0000gn/T/ipykernel_82873/2810039421.py:105: FutureWarning: Slicing a positional slice with .loc is not supported, and will raise TypeError in a future version.  Use .loc with labels or .iloc with positions instead.\n",
      "  output_df.loc[:,1:] = output_df.iloc[:,1:].applymap(binarize)\n",
      "/var/folders/g0/54cppbx96dj9w8tzkq98nqsw0000gn/T/ipykernel_82873/2810039421.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  output_df.loc[:,1:] = output_df.iloc[:,1:].applymap(binarize)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>petroleum_based</th>\n",
       "      <th>bottle</th>\n",
       "      <th>other</th>\n",
       "      <th>paper</th>\n",
       "      <th>Clean</th>\n",
       "      <th>Indoor,_Man-made</th>\n",
       "      <th>Pavement</th>\n",
       "      <th>Sand,_Dirt,_Pebbles</th>\n",
       "      <th>Trash</th>\n",
       "      <th>Vegetation</th>\n",
       "      <th>Water</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>batch_1_000006.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>batch_1_000008.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>batch_1_000010.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>batch_1_000019.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>batch_1_000026.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            file_name  petroleum_based  bottle  other  paper  Clean  \\\n",
       "0  batch_1_000006.jpg                0       1      0      0      0   \n",
       "1  batch_1_000008.jpg                0       0      0      1      0   \n",
       "2  batch_1_000010.jpg                1       1      0      0      0   \n",
       "3  batch_1_000019.jpg                1       1      0      0      0   \n",
       "4  batch_1_000026.jpg                0       1      0      0      0   \n",
       "\n",
       "   Indoor,_Man-made  Pavement  Sand,_Dirt,_Pebbles  Trash  Vegetation  Water  \n",
       "0                 1         0                    0      0           0      0  \n",
       "1                 1         0                    0      0           0      0  \n",
       "2                 0         1                    0      0           0      0  \n",
       "3                 0         0                    0      0           1      0  \n",
       "4                 0         0                    0      0           1      0  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labels = pd.read_csv('./data/splits/labels.csv')\n",
    "\n",
    "def feature_engineering(input_df, merging_features, new_feature_name):\n",
    "    '''\n",
    "    New definition of a super category:\n",
    "    1. Input the original labels df\n",
    "    2. Enter a list of labels that should be merged together to a new super category\n",
    "    3. Give the new super category a name\n",
    "    '''\n",
    "    output_df = input_df.copy(deep=True)\n",
    "    \n",
    "    output_df[str(new_feature_name)] = output_df[merging_features].any(axis=1)\n",
    "    \n",
    "    return output_df[str(new_feature_name)]\n",
    "\n",
    "petroleum_based = ['Plastic_bag_&_wrapper',\n",
    "                   'Bottle_cap',\n",
    "                   'Other_plastic',\n",
    "                   'Straw',\n",
    "                   'Lid',\n",
    "                   'Plastic_container',\n",
    "                   'Plastic_utensils',\n",
    "                   'Rope_&_strings',\n",
    "                   'Blister_pack',\n",
    "                   'Plastic_glooves']\n",
    "                   \n",
    "                   \n",
    "                   \n",
    "\n",
    "bottle = ['Bottle',\n",
    "          'Can',\n",
    "          'Glass_jar'\n",
    "          ]\n",
    "\n",
    "\n",
    "other = ['Unlabeled_litter',\n",
    "         'Cigarette',\n",
    "         'Styrofoam_piece',\n",
    "         'Pop_tab',\n",
    "         'Broken_glass',\n",
    "         'Scrap_metal',\n",
    "         'Food_waste',\n",
    "         'Shoe',\n",
    "         'Squeezable_tube',\n",
    "         'Battery']\n",
    "         \n",
    "         \n",
    "\n",
    "\n",
    "paper = ['Carton',\n",
    "         'Cup',\n",
    "         'Paper',\n",
    "         'Paper_bag'\n",
    "         ]\n",
    "\n",
    "scene_1 = ['Clean']\n",
    "scene_2 = ['Indoor,_Man-made']\n",
    "scene_3 = ['Pavement']\n",
    "scene_4 = ['Sand,_Dirt,_Pebbles']\n",
    "scene_5 = ['Trash']\n",
    "scene_6 = ['Vegetation']\n",
    "scene_7 = ['Water']\n",
    "\n",
    "\n",
    "recycle_scenes_dict = {\n",
    "    'petroleum_based': petroleum_based,\n",
    "    'bottle': bottle,\n",
    "    'other': other,\n",
    "    'paper':paper,\n",
    "    'Clean':scene_1,\n",
    "    'Indoor,_Man-made':scene_2,\n",
    "    'Pavement':scene_3,\n",
    "    'Sand,_Dirt,_Pebbles':scene_4,\n",
    "    'Trash':scene_5,\n",
    "    'Vegetation':scene_6,\n",
    "    'Water':scene_7\n",
    "        \n",
    "}\n",
    "\n",
    "\n",
    "def recycle_labels(input_df, feature_definitions_dict):\n",
    "    '''\n",
    "    1. Create a dict with the new label name as the key and the merging features (list) as values\n",
    "    2. Give the csv a new name\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    list_labels = ['file_name']\n",
    "    df_new_labels = input_df.copy(deep=True)\n",
    "    \n",
    "    for key, value in feature_definitions_dict.items():\n",
    "        df_feature = feature_engineering(input_df, merging_features=value, new_feature_name=key)\n",
    "        df_new_labels[key] = df_feature\n",
    "        list_labels.append(key)\n",
    "        \n",
    "\n",
    "    \n",
    "    output_df = df_new_labels[list_labels]\n",
    "    \n",
    "    def binarize(x):\n",
    "        if x == True:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    output_df.loc[:,1:] = output_df.iloc[:,1:].applymap(binarize)\n",
    " \n",
    "    \n",
    "    output_df.to_csv('data/splits/optimized_scenes_labels.csv',index=False)\n",
    "    \n",
    "    return output_df\n",
    "    \n",
    "rec_labels = recycle_labels(labels,recycle_scenes_dict)   \n",
    "rec_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a571814e-ad9d-42a9-93b6-3d72b1f4b8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1496, 12)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf3d2d2-395b-40d0-be08-d0d52924e609",
   "metadata": {},
   "source": [
    "#### Section 4: Split and balance the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1f1845fa-9898-4dde-886d-d1d969ff1c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0, 'name': 'Clean'},\n",
       " {'id': 1, 'name': 'Indoor, Man-made'},\n",
       " {'id': 2, 'name': 'Pavement'},\n",
       " {'id': 3, 'name': 'Sand, Dirt, Pebbles'},\n",
       " {'id': 4, 'name': 'Trash'},\n",
       " {'id': 5, 'name': 'Vegetation'},\n",
       " {'id': 6, 'name': 'Water'}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2d526248-1fa6-423c-85a0-7aeb6d893c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Original train: \t 908 rows\n",
      "    Original validation:   243 rows\n",
      "    Original test: \t   345 rows\n",
      "    \n",
      "    --Row count summary--\n",
      "    Stratified split 85-15\n",
      "    Train  \t 908 rows\n",
      "    Validation    243 rows\n",
      "    \n",
      "    Partial balancing\n",
      "    Train bal\t 1518 rows\n",
      "    \n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#Check which column is the file_name column\n",
    "rec_df = pd.read_csv('./data/splits/optimized_scenes_labels.csv')\n",
    "labels_orig = list(rec_df.columns)[1:]\n",
    "\n",
    "def stratified_split (df, train_split, val_split, columns):\n",
    "    assert (train_split + val_split) == 1\n",
    "    \n",
    "    df_sample = df.sample(frac=1, random_state=42)\n",
    "\n",
    "    grouped_df = df_sample.groupby(columns)\n",
    "    arr_list = [np.split(g, [int(train_split * len(g))]) for i, g in grouped_df]\n",
    "    \n",
    "    train_df = pd.concat([t[0] for t in arr_list])\n",
    "    val_df = pd.concat([t[1] for t in arr_list])\n",
    "         \n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "train_df, test_df = stratified_split(df = rec_df, train_split = 0.85, val_split = 0.15, columns = labels_orig )\n",
    "train_df, val_df = stratified_split(df = train_df, train_split = 0.85, val_split = 0.15, columns = labels_orig )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def class_balancing (input_df, classes_to_balance):\n",
    "\n",
    "    mygrpCounts = input_df.groupby(classes_to_balance)[\"file_name\"].count()\n",
    "    myindxs = mygrpCounts.index.to_numpy()\n",
    "    vals = mygrpCounts.values.astype(np.float32)\n",
    "    #vals2=np.minimum(np.sqrt(vals.max()/(vals)), 7)\n",
    "    vals2 = (vals.max()/(vals))**0.3 # this limits the oversampling since 1:1 would lead to a 8x increase in size\n",
    "    vals2 = vals2.round().astype(np.int32)\n",
    "    output_df = input_df.copy(deep=True)\n",
    "    \n",
    "    for index in range(len(vals2)):\n",
    "        locconds=myindxs[index]\n",
    "        locval=vals2[index]\n",
    "        locDf=input_df\n",
    "        for colmnInd in range(len(classes_to_balance)):\n",
    "            locDf=locDf[locDf[classes_to_balance[colmnInd]]==locconds[colmnInd]]\n",
    "        if vals2[index]!=1:\n",
    "            locDf2=pd.concat([locDf]*(locval-1), ignore_index=True)\n",
    "            output_df=pd.concat((output_df, locDf2), ignore_index=True)\n",
    "\n",
    "    output_df=output_df.sample(frac=1)\n",
    "    #newcounts=output_df.groupby(classes_to_balance)[\"name\"].count()\n",
    "    #print(newcounts)\n",
    "    #print(mygrpCounts)\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "train_rec_bal = class_balancing(input_df = train_df, classes_to_balance = labels_orig)\n",
    "\n",
    "\n",
    "CSVPathIn = './data/splits/'\n",
    "\n",
    "testPathOut=f'{CSVPathIn}test_opt_scene_2023-05-08.csv'\n",
    "trainbalPathOut=f'{CSVPathIn}train_opt_scene_partially_balanced_2023-05-08.csv'\n",
    "trainPathOut=f'{CSVPathIn}train_opt_scene_2023-05-08.csv'\n",
    "valPathOut=f'{CSVPathIn}val_opt_scene_2023-05-08.csv'\n",
    "\n",
    "test_df.to_csv(testPathOut,index=False)\n",
    "train_rec_bal.to_csv(trainbalPathOut,index=False)\n",
    "train_df.to_csv(trainPathOut,index=False)\n",
    "val_df.to_csv(valPathOut,index=False)\n",
    "\n",
    "print(f'''\n",
    "    Original train: \\t {train_df.shape[0]} rows\n",
    "    Original validation:   {val_df.shape[0]} rows\n",
    "    Original test: \\t   {test_df.shape[0]} rows\n",
    "    \n",
    "    --Row count summary--\n",
    "    Stratified split 85-15\n",
    "    Train  \\t {train_df.shape[0]} rows\n",
    "    Validation    {val_df.shape[0]} rows\n",
    "    \n",
    "    Partial balancing\n",
    "    Train bal\\t {train_rec_bal.shape[0]} rows\n",
    "    \n",
    "    \n",
    "    ''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66254666-7303-4f59-9506-949d9a42c8a7",
   "metadata": {},
   "source": [
    "### K-Fold - increasing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78abe7e2-d033-4b41-b114-f307e40a289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bal_mat = pd.read_csv('./data/splits/train_material_partially_balanced_2023-04-13.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d0b68c4-85d7-4d06-9a54-3620f4848185",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVPathIn = './data/splits/'\n",
    "trainbalx3PathOut=f'{CSVPathIn}train_material_partially_balanced_3fold_2023-04-14.csv'\n",
    "trainbalx5PathOut=f'{CSVPathIn}train_material_partially_balanced_5_fold_2023-04-14.csv'\n",
    "\n",
    "\n",
    "\n",
    "train_balx3_df = pd.concat([train_bal_mat]*3, ignore_index=True)\n",
    "train_balx5_df = pd.concat([train_bal_mat]*5, ignore_index=True)\n",
    "\n",
    "\n",
    "train_balx3_df.to_csv(trainbalx3PathOut,index=False)\n",
    "train_balx5_df.to_csv(trainbalx5PathOut,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65126e1e-ae84-4309-a154-336fcce91ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLG",
   "language": "python",
   "name": "dlg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
